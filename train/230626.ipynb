{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "# 불용어 리스트\n",
    "languages = [\n",
    "   'English'\n",
    "]\n",
    "my_stop_word_list = []\n",
    "for i in languages:\n",
    "    my_stop_word_list.append(get_stop_words(i.lower()))\n",
    "my_stop_word_list = sum(my_stop_word_list, [])\n",
    "my_stop_word_list = my_stop_word_list + ['paperbook', 'hardcover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/nealrs/96342d8231b75cf4bb82\n",
    "contractions = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#main 단어만 추출 > 차후 이 단어들로 test\n",
    "def preprocessing_sentence(sentence, remove_stopwords = True):\n",
    "    #소문자화\n",
    "    sentence = sentence.lower()\n",
    "    #괄호로 닫친 문자열 괄호 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)',' ',sentence)\n",
    "    #쌍따옴표 제거\n",
    "    sentence = re.sub('\"', ' ', sentence) \n",
    "    #약어 정규화\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")])\n",
    "    #소유격 제거\n",
    "    sentence = re.sub(r\"'s\\b\",\" \",sentence)\n",
    "    #특수문자 제거\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "    \n",
    "    #불용어 제거\n",
    "    if my_stop_word_list:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in my_stop_word_list if len(word)>1)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word)>1)\n",
    "    return tokens\n",
    "data['facts'] = [preprocessing_sentence(i) for i in data['facts']]\n",
    "test['facts'] = [preprocessing_sentence(i) for i in test['facts']]\n",
    "\n",
    "data['facts'] = data['facts'].astype('str')\n",
    "test['facts'] = test['facts'].astype('str')\n",
    "\n",
    "facts_corpus_lose = ' '.join(data[data['first_party_winner'] == 0]['facts'])\n",
    "facts_corpus_win = ' '.join(data[data['first_party_winner'] == 1]['facts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kweon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>june phil st amant candidate public office mad...</td>\n",
       "      <td>1</td>\n",
       "      <td>public sued first circuit court appeals revers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>ramon nelson riding bike suffered lethal blow ...</td>\n",
       "      <td>0</td>\n",
       "      <td>two convicted death judge trial ruled also fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>alabama state court convicted billy joe magwoo...</td>\n",
       "      <td>1</td>\n",
       "      <td>state court convicted murder sentenced death f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>victor linkletter convicted state court eviden...</td>\n",
       "      <td>0</td>\n",
       "      <td>convicted state court evidence police supreme ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>april selma alabama intruder broke apartment d...</td>\n",
       "      <td>1</td>\n",
       "      <td>city several trial police held police later ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>TRAIN_2473</td>\n",
       "      <td>HollyFrontier Cheyenne Refining, LLC, et al.</td>\n",
       "      <td>Renewable Fuels Association, et al.</td>\n",
       "      <td>congress amended clean air act energy policy a...</td>\n",
       "      <td>1</td>\n",
       "      <td>act act several one year three alleging circui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>TRAIN_2474</td>\n",
       "      <td>Grupo Mexicano de Desarrollo, S. A.</td>\n",
       "      <td>Alliance Bond Fund, Inc.</td>\n",
       "      <td>alliance bond fund inc investment fund purchas...</td>\n",
       "      <td>1</td>\n",
       "      <td>holding company government filed suit federal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>TRAIN_2475</td>\n",
       "      <td>Peguero</td>\n",
       "      <td>United States</td>\n",
       "      <td>district court sentenced manuel peguero months...</td>\n",
       "      <td>0</td>\n",
       "      <td>district court sentenced guilty federal court ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>TRAIN_2476</td>\n",
       "      <td>Immigration and Naturalization Service</td>\n",
       "      <td>St. Cyr</td>\n",
       "      <td>march enrico st cyr lawful permanent resident ...</td>\n",
       "      <td>0</td>\n",
       "      <td>guilty court conviction death act act section ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>TRAIN_2477</td>\n",
       "      <td>Markman</td>\n",
       "      <td>Westview Instruments, Inc.</td>\n",
       "      <td>herbert markman owns patent system tracks clot...</td>\n",
       "      <td>0</td>\n",
       "      <td>claim rights also suit claim jury found howeve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                   first_party  \\\n",
       "0     TRAIN_0000                             Phil A. St. Amant   \n",
       "1     TRAIN_0001                                Stephen Duncan   \n",
       "2     TRAIN_0002                             Billy Joe Magwood   \n",
       "3     TRAIN_0003                                    Linkletter   \n",
       "4     TRAIN_0004                            William Earl Fikes   \n",
       "...          ...                                           ...   \n",
       "2473  TRAIN_2473  HollyFrontier Cheyenne Refining, LLC, et al.   \n",
       "2474  TRAIN_2474           Grupo Mexicano de Desarrollo, S. A.   \n",
       "2475  TRAIN_2475                                       Peguero   \n",
       "2476  TRAIN_2476        Immigration and Naturalization Service   \n",
       "2477  TRAIN_2477                                       Markman   \n",
       "\n",
       "                             second_party  \\\n",
       "0                      Herman A. Thompson   \n",
       "1                          Lawrence Owens   \n",
       "2          Tony Patterson, Warden, et al.   \n",
       "3                                  Walker   \n",
       "4                                 Alabama   \n",
       "...                                   ...   \n",
       "2473  Renewable Fuels Association, et al.   \n",
       "2474             Alliance Bond Fund, Inc.   \n",
       "2475                        United States   \n",
       "2476                              St. Cyr   \n",
       "2477           Westview Instruments, Inc.   \n",
       "\n",
       "                                                  facts  first_party_winner  \\\n",
       "0     june phil st amant candidate public office mad...                   1   \n",
       "1     ramon nelson riding bike suffered lethal blow ...                   0   \n",
       "2     alabama state court convicted billy joe magwoo...                   1   \n",
       "3     victor linkletter convicted state court eviden...                   0   \n",
       "4     april selma alabama intruder broke apartment d...                   1   \n",
       "...                                                 ...                 ...   \n",
       "2473  congress amended clean air act energy policy a...                   1   \n",
       "2474  alliance bond fund inc investment fund purchas...                   1   \n",
       "2475  district court sentenced manuel peguero months...                   0   \n",
       "2476  march enrico st cyr lawful permanent resident ...                   0   \n",
       "2477  herbert markman owns patent system tracks clot...                   0   \n",
       "\n",
       "                                          filtered_text  \n",
       "0     public sued first circuit court appeals revers...  \n",
       "1     two convicted death judge trial ruled also fou...  \n",
       "2     state court convicted murder sentenced death f...  \n",
       "3     convicted state court evidence police supreme ...  \n",
       "4     city several trial police held police later ti...  \n",
       "...                                                 ...  \n",
       "2473  act act several one year three alleging circui...  \n",
       "2474  holding company government filed suit federal ...  \n",
       "2475  district court sentenced guilty federal court ...  \n",
       "2476  guilty court conviction death act act section ...  \n",
       "2477  claim rights also suit claim jury found howeve...  \n",
       "\n",
       "[2478 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# 데이터프레임에서 문장 추출\n",
    "sentences = data['facts'].tolist()\n",
    "\n",
    "# 문장을 단어로 토큰화\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# 모든 문장에서의 단어 빈도수 합산\n",
    "word_frequencies = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    word_frequencies.update(sentence)\n",
    "\n",
    "# 1000개 이하의 빈도수를 가진 단어 제거하여 문장 재구성\n",
    "filtered_sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    filtered_sentence = ' '.join([word for word in sentence if word_frequencies[word] > 300])\n",
    "    filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "# 필터링된 문장들을 데이터프레임 열에 할당\n",
    "data['filtered_text'] = filtered_sentences\n",
    "\n",
    "# 결과 확인\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States       154\n",
       "Illinois              9\n",
       "Maryland              8\n",
       "Florida               8\n",
       "New York              7\n",
       "                   ... \n",
       "David Carpenter       1\n",
       "Larry Gene Heath      1\n",
       "PGA TOUR, Inc.        1\n",
       "PPL Montana, LLC      1\n",
       "Markman               1\n",
       "Name: first_party, Length: 2110, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['first_party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States                        240\n",
       "California                            19\n",
       "United States of America              15\n",
       "Illinois                              13\n",
       "Federal Communications Commission     10\n",
       "                                    ... \n",
       "David Boren, Governor of Oklahoma      1\n",
       "Federal Bureau of Prisons et al.       1\n",
       "Town of Harrison                       1\n",
       "Charles Burr et al.                    1\n",
       "Westview Instruments, Inc.             1\n",
       "Name: second_party, Length: 1974, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['second_party'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "subset_0 = data[data[\"first_party_winner\"] == 0]\n",
    "subset_1 = data[data[\"first_party_winner\"] == 1]\n",
    "\n",
    "subset_1_downsampled = resample(subset_1,\n",
    "                                replace=False,\n",
    "                                n_samples= 829,\n",
    "                                random_state=42)\n",
    "\n",
    "data = pd.concat([subset_0, subset_1_downsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "def get_vector(vectorizer, df, train_mode):\n",
    "    if train_mode:\n",
    "        X_filtered_text = vectorizer.fit_transform(df['filtered_text'])\n",
    "    else:\n",
    "        X_filtered_text = vectorizer.transform(df['facts'])\n",
    "    X_party1 = vectorizer.transform(df['first_party'])\n",
    "    X_party2 = vectorizer.transform(df['second_party'])\n",
    "    \n",
    "    X = np.concatenate([X_party1.todense(), X_party2.todense(), X_filtered_text.todense()], axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_vector(vectorizer, data, True)\n",
    "Y = data[\"first_party_winner\"]\n",
    "X_T = get_vector(vectorizer, test, False)\n",
    "\n",
    "X = np.squeeze(np.asarray(X))\n",
    "X_T = np.squeeze(np.asarray(X_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 294)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_T.shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,3))\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_T_scaled = scaler.transform(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42 , stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\n",
    "                'verbose' : 0,\n",
    "                'random_state': 113,\n",
    "               # 'use_best_model' : True,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"KNN Classifier\",\n",
    "    \"Decision Tree\",\n",
    "    \"LinearSVC\",\n",
    "    \"Linear SVM\",\n",
    "    \"Poly SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"sigmoid SVM\",\n",
    "    \"Neural Network\",\n",
    "    \"Random Forest\",\n",
    "    \"SGD Classifier\",\n",
    "    \"Ridge Classifier\",\n",
    "    \"XGBoost\",\n",
    "    \"AdaBoost\",\n",
    "    \"Catboost\",\n",
    "    \"Gaussian Naive Bayes\"\n",
    "]\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter = 1000),\n",
    "    KNeighborsClassifier(n_neighbors = 149, n_jobs = -1),\n",
    "    DecisionTreeClassifier(),\n",
    "    LinearSVC(),\n",
    "    svm.SVC(kernel = 'linear'),\n",
    "    svm.SVC(kernel = 'poly'),\n",
    "    svm.SVC(kernel = 'rbf'),\n",
    "    svm.SVC(kernel = 'sigmoid'),\n",
    "    MLPClassifier(max_iter=300),\n",
    "    RandomForestClassifier(n_estimators = 100),\n",
    "    SGDClassifier(loss = 'hinge'),\n",
    "    RidgeClassifier(),\n",
    "    XGBClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    CatBoostClassifier(**cat_params),\n",
    "    GaussianNB()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Function to return summary of baseline models\n",
    "def score(X_train, y_train, X_val, y_val, names = names, models = models):\n",
    "    score_df, score_train, score_val = pd.DataFrame(), [], []\n",
    "    x = time.time()\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred, y_val_pred = model.predict(X_train), model.predict(X_val)\n",
    "        score_train.append(accuracy_score(y_train, y_train_pred))\n",
    "        score_val.append(accuracy_score(y_val, y_val_pred))\n",
    "    \n",
    "    score_df[\"Classifier\"], score_df[\"Training accuracy\"], score_df[\"Validation accuracy\"] = names, score_train, score_val\n",
    "    score_df.sort_values(by = 'Validation accuracy', ascending = False, inplace = True)\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Training accuracy</th>\n",
       "      <th>Validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Poly SVM</td>\n",
       "      <td>0.952489</td>\n",
       "      <td>0.608434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.963801</td>\n",
       "      <td>0.575301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RBF SVM</td>\n",
       "      <td>0.838612</td>\n",
       "      <td>0.557229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.996229</td>\n",
       "      <td>0.557229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.999246</td>\n",
       "      <td>0.548193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.700603</td>\n",
       "      <td>0.548193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.999246</td>\n",
       "      <td>0.545181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sigmoid SVM</td>\n",
       "      <td>0.567119</td>\n",
       "      <td>0.539157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.608597</td>\n",
       "      <td>0.533133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.668929</td>\n",
       "      <td>0.527108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.670437</td>\n",
       "      <td>0.524096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.673454</td>\n",
       "      <td>0.524096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.521084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.503012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.523379</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.999246</td>\n",
       "      <td>0.487952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Classifier  Training accuracy  Validation accuracy\n",
       "5               Poly SVM           0.952489             0.608434\n",
       "14              Catboost           0.963801             0.575301\n",
       "6                RBF SVM           0.838612             0.557229\n",
       "12               XGBoost           0.996229             0.557229\n",
       "9          Random Forest           0.999246             0.548193\n",
       "13              AdaBoost           0.700603             0.548193\n",
       "8         Neural Network           0.999246             0.545181\n",
       "7            sigmoid SVM           0.567119             0.539157\n",
       "1         KNN Classifier           0.608597             0.533133\n",
       "11      Ridge Classifier           0.668929             0.527108\n",
       "0    Logistic Regression           0.670437             0.524096\n",
       "3              LinearSVC           0.673454             0.524096\n",
       "4             Linear SVM           0.676471             0.521084\n",
       "10        SGD Classifier           0.607843             0.503012\n",
       "15  Gaussian Naive Bayes           0.523379             0.500000\n",
       "2          Decision Tree           0.999246             0.487952"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(X_train, y_train, X_test, y_test, names = names, models = models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kweon\\anaconda3\\envs\\gibo\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>563</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>538</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>603</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>558</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>574</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Poly SVM</td>\n",
       "      <td>570</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RBF SVM</td>\n",
       "      <td>558</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sigmoid SVM</td>\n",
       "      <td>554</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>625</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>599</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>620</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>565</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>594</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>602</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Catboost</td>\n",
       "      <td>596</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>107</td>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Classifier    0     1\n",
       "0    Logistic Regression  563   677\n",
       "1         KNN Classifier  538   702\n",
       "2          Decision Tree  603   637\n",
       "3              LinearSVC  558   682\n",
       "4             Linear SVM  574   666\n",
       "5               Poly SVM  570   670\n",
       "6                RBF SVM  558   682\n",
       "7            sigmoid SVM  554   686\n",
       "8         Neural Network  625   615\n",
       "9          Random Forest  599   641\n",
       "10        SGD Classifier  620   620\n",
       "11      Ridge Classifier  565   675\n",
       "12               XGBoost  594   646\n",
       "13              AdaBoost  602   638\n",
       "14              Catboost  596   644\n",
       "15  Gaussian Naive Bayes  107  1133"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pred_score(X, Y, X_T, names, models):\n",
    "    score_df = pd.DataFrame()\n",
    "    pred_0, pred_1 = [], []\n",
    "    for model in models:\n",
    "        model.fit(X, Y)\n",
    "        pred = model.predict(X_T)\n",
    "        pred_0.append(pd.DataFrame(pred).value_counts().to_frame().T[0].values[0][0])\n",
    "        pred_1.append(pd.DataFrame(pred).value_counts().to_frame().T[1].values[0][0])\n",
    "    score_df[\"Classifier\"], score_df[\"0\"], score_df[\"1\"] = names, pred_0, pred_1\n",
    "    return score_df    \n",
    "\n",
    "pred_score(X=X_scaled, Y=Y, X_T=X_T_scaled, names=names, models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KNeighborsClassifier(n_neighbors = 149, n_jobs = -1)\n",
    "# model = svm.SVC(kernel = 'poly')\n",
    "model = svm.SVC(kernel = 'sigmoid')\n",
    "model.fit(X_scaled, Y)\n",
    "pred = model.predict(X_T_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred).value_counts().to_frame().T[1].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    686\n",
       "0    554\n",
       "Name: first_party_winner, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['first_party_winner']= pred\n",
    "sub['first_party_winner'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('del300_downsampling_scaling_svmpoly_230626.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gibo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

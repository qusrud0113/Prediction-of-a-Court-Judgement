{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a31e43-a4c7-4bf1-be45-d15f445374bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "for package in [\"transformers==4.17.0\",\n",
    "\"sentencepiece==0.1.97\",\n",
    "\"torch==1.11.0+cu113\",\n",
    "\"tokenizers==0.12.1\",\n",
    "\"spacy==3.2.3\",\n",
    "\"scikit-learn==1.1.1\",\n",
    "\"pandas==1.3.4\",\n",
    "\"numpy==1.20.3\",\n",
    "\"nltk==3.6.5\",\n",
    "\"matplotlib==3.4.3\",\n",
    "\"datasets==2.6.1\"]:\n",
    "  subprocess.run(f\"pip install {package}\", shell=True)\n",
    "subprocess.run(\"python -m spacy download en_core_web_sm\", shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b644f612-ff92-4f63-91cf-c1ecb6e836e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 15:14:21.939226: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 15:14:22.525845: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/chunbae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/chunbae/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/chunbae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import datetime\n",
    "import spacy\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import hashlib\n",
    "import datasets\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers.tokenization_utils import TruncationStrategy\n",
    "\n",
    "## bert\n",
    "from transformers import BertTokenizer, BertModel,BertTokenizerFast\n",
    "## longformer\n",
    "from transformers import LongformerModel, LongformerTokenizer,LongformerTokenizerFast\n",
    "\n",
    "##bigbert\n",
    "from transformers import BigBirdTokenizer, BigBirdModel,BigBirdTokenizerFast\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam,AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score,matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc6639d-67e7-425a-bb6d-3db0338afca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5a1344-3e54-4c32-a628-2dbd3b4f53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f8a176-2e3b-40c3-84cc-21577f95eace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2478 entries, 0 to 2477\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   ID                  2478 non-null   object\n",
      " 1   first_party         2478 non-null   object\n",
      " 2   second_party        2478 non-null   object\n",
      " 3   facts               2478 non-null   object\n",
      " 4   first_party_winner  2478 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 96.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b3bcbe-5ea9-4f5d-ab26-67b0f65c9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def stem_text_func(text):\n",
    "    token_words=word_tokenize(text)\n",
    "    stem_sentence = []\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "train_df['facts'] =train_df.facts.apply(lambda x : stem_text_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4199b7a-e2d9-4917-85d5-f88f24ae7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['facts'] =test_df.facts.apply(lambda x : stem_text_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29a8267e-59dc-4a5d-a9bb-5ecf9302a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tok_len(text,tokenizer):\n",
    "  inputs = tokenizer(text, return_tensors=\"pt\",padding=True)\n",
    "  return len(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c09b278f-17f1-4c7f-aac9-a7a39dc9445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_counter(c: Counter) -> Counter:\n",
    "    total = sum(c.values(), 0.0)\n",
    "    for key in c:\n",
    "        c[key] /= total\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbfbe6d8-aaa6-4427-bd9f-8ef84ba3d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_hash(dictionary: Dict[str, Any]) -> str:\n",
    "    \"\"\"MD5 hash of a dictionary.\"\"\"\n",
    "    dhash = hashlib.md5()\n",
    "    encoded = json.dumps(dictionary, sort_keys=True).encode()\n",
    "    dhash.update(encoded)\n",
    "    return dhash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82fe6566-d472-45b6-9c83-621e1089e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(results,epoch_num,origin_labels,predict_outputs,dataset='test'):\n",
    "    results[epoch_num + 1][dataset] = {}\n",
    "    results[epoch_num + 1][dataset]['accuracy'] = accuracy_score(origin_labels, predict_outputs)\n",
    "\n",
    "    results[epoch_num + 1][dataset]['recall_weighted'] = recall_score(origin_labels, predict_outputs, average='weighted')\n",
    "    results[epoch_num + 1][dataset]['recall_micro'] = recall_score(origin_labels, predict_outputs, average='micro')\n",
    "    results[epoch_num + 1][dataset]['recall_macro'] = recall_score(origin_labels, predict_outputs, average='macro')\n",
    "\n",
    "    results[epoch_num + 1][dataset]['precision_weighted'] = precision_score(origin_labels, predict_outputs, average='weighted')\n",
    "    results[epoch_num + 1][dataset]['precision_micro'] = precision_score(origin_labels, predict_outputs, average='micro')\n",
    "    results[epoch_num + 1][dataset]['precision_macro'] = precision_score(origin_labels, predict_outputs, average='macro')\n",
    "    results[epoch_num + 1][dataset]['classification_report'] = classification_report(origin_labels, predict_outputs, target_names=['lose','win'])\n",
    "    results[epoch_num + 1][dataset]['confusion_matrix'] =  confusion_matrix(origin_labels, predict_outputs).tolist()\n",
    "    results[epoch_num + 1][dataset]['matthews_corrcoef'] =  matthews_corrcoef(origin_labels, predict_outputs)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68a12a5-cbfb-45a5-9093-ee4e97519e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_match_len(df_train,df_test,df_val,tokenizer,max_length):\n",
    "    df_train['num_of_bert_token'] = df_train[TEXT_COL].apply(lambda x : bert_tok_len(x,tokenizer))\n",
    "    df_test['num_of_bert_token'] = df_test[TEXT_COL].apply(lambda x : bert_tok_len(x,tokenizer))\n",
    "    df_val['num_of_bert_token'] = df_val[TEXT_COL].apply(lambda x : bert_tok_len(x,tokenizer))\n",
    "    df_train = df_train[df_train['num_of_bert_token']<=max_length]\n",
    "    df_test = df_test[df_test['num_of_bert_token']<=max_length]\n",
    "    df_val = df_val[df_val['num_of_bert_token']<=max_length]\n",
    "    test = df_test.copy(deep=True)\n",
    "    train = df_train.copy(deep=True)\n",
    "    val = df_val.copy(deep=True)\n",
    "    return train,test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd1059c-90d2-4746-90e2-d1e2296314c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, label_col, text_col, max_length, tokenizer):\n",
    "        self.labels = df[label_col].values\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = max_length, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df[text_col]]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba3243a-c6fa-4f4e-b3b5-adf8a289046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_FUNCTIONS = {\n",
    "    'Softmax' :nn.Softmax(),\n",
    "    'LeakyRelu' : nn.LeakyReLU(),\n",
    "    'Relu':nn.ReLU(),\n",
    "      'GELU':nn.GELU()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8ed542-a506-436d-97dc-9e663f44065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, model, num_labels, num_features, dropout=0.001,freeze_layers = False,activation_func = 'Softmax'):\n",
    "      \n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "                \n",
    "        if freeze_layers:\n",
    "            for layer in self.model.encoder.layer[:-2]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(num_features, num_labels)\n",
    "        self.activation_layer = ACTIVATION_FUNCTIONS.get(activation_func)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.model(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.activation_layer(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "843b38b7-f4e9-4304-9d1e-4db9398bcf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model_obj, tokenizer, train_data, val_data, test_data, config,parameters,model_num,model_name='',accum_iter = 16,early_stopping = True,weight_decay=0 ):\n",
    "\n",
    "\n",
    "    the_last_loss = 100\n",
    "    triggertimes = 0\n",
    "    patience = 2 if  early_stopping else config['epochs']\n",
    "    \n",
    "    \n",
    "    train, val, test = Dataset(train_data, config['label_col'], config['text_col'], config['max_length'], tokenizer), \\\n",
    "                       Dataset(val_data, config['label_col'], config['text_col'], config['max_length'], tokenizer), \\\n",
    "                       Dataset(test_data, config['label_col'], config['text_col'], config['max_length'], tokenizer)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=config['batch_size'])\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=config['batch_size'])\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    model = model_obj\n",
    "    \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=parameters['learning_rate'],weight_decay = parameters['weight_decay'])\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "    results = {}\n",
    "    for epoch_num in range(config['epochs']):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "                            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            train_labels = []\n",
    "            train_outputs = []\n",
    "            test_labels = []\n",
    "            test_outputs = []\n",
    "            val_labels = []\n",
    "            val_outputs = []\n",
    "\n",
    "            \n",
    "            scaler = torch.cuda.amp.GradScaler() \n",
    "            \n",
    "            for batch_idx,( train_text_input, train_label) in  enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_text_input['attention_mask'].to(device)\n",
    "                input_id = train_text_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast(): \n",
    "                    output = model(input_id, mask)\n",
    "                \n",
    "                    batch_loss = criterion(output, train_label)\n",
    "                \n",
    "                    batch_loss = batch_loss / accum_iter\n",
    "                \n",
    "                    total_loss_train += batch_loss.item()\n",
    "                \n",
    "                    acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                    total_acc_train += acc\n",
    "                \n",
    "                    train_labels.append(train_label.cpu().tolist())\n",
    "                    train_outputs.append(output.argmax(dim=1).cpu().tolist())\n",
    "                \n",
    "\n",
    "                if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
    "                    model.zero_grad()\n",
    "                    scaler.scale(batch_loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    \n",
    "                    \n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "                    val_labels.append(val_label.cpu().tolist())\n",
    "                    val_outputs.append(output.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for test_input, test_label in test_dataloader:\n",
    "\n",
    "                    test_label = test_label.to(device)\n",
    "                    mask = test_input['attention_mask'].to(device)\n",
    "                    input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    test_labels.append(test_label.cpu().tolist())\n",
    "                    test_outputs.append(output.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "        \n",
    "            val_labels = [item for sublist in val_labels for item in sublist]\n",
    "            val_outputs = [item for sublist in val_outputs for item in sublist]\n",
    "            \n",
    "            test_labels = [item for sublist in test_labels for item in sublist]\n",
    "            test_outputs = [item for sublist in test_outputs for item in sublist]\n",
    "            \n",
    "            train_labels = [item for sublist in train_labels for item in sublist]\n",
    "            train_outputs = [item for sublist in train_outputs for item in sublist]\n",
    "            \n",
    "            \n",
    "            results[epoch_num + 1] = {'train_loss': total_loss_train / len(train_data),\n",
    "                                     'val_loss': total_loss_val / len(val_data),\n",
    "                                     'train_acc': total_acc_train / len(train_data),\n",
    "                                     'val_acc': total_acc_val / len(val_data)}\n",
    "            \n",
    "            \n",
    "            results = get_stats(results,epoch_num,train_labels,train_outputs,dataset='train')\n",
    "            results = get_stats(results,epoch_num,val_labels,val_outputs,dataset='val')\n",
    "            results = get_stats(results,epoch_num,test_labels,test_outputs,dataset='test')\n",
    "\n",
    "\n",
    "            \n",
    "            #### Early Stopping ####\n",
    "            \n",
    "            current_loss = total_loss_val / len(val_data)\n",
    "            if current_loss > the_last_loss:\n",
    "                trigger_times += 1\n",
    "                print('Trigger Times:', trigger_times)\n",
    "\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early Stopping!\\nStart to test process.')\n",
    "                    return epoch_num + 1 ,results\n",
    "\n",
    "            else:\n",
    "                print('Trigger Times: 0')\n",
    "                trigger_times = 0\n",
    "\n",
    "            the_last_loss = current_loss\n",
    "            \n",
    "            \n",
    "    return epoch_num + 1 ,results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f2263dd-b902-46f4-a8c2-709f332ae9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL =   'facts' \n",
    "FILTER_LEN = 'truncate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7632e30f-3895-4f0b-b23c-1f590fe3bb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd55bb1989af4770a9c1574a59ef53f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f09a50660ed4389a6746d0c4805c856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 512\n",
    "\n",
    "models_conf = {\n",
    "    f'LegalBert_{MAX_LENGTH}_1': \n",
    "    {'model': BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased',force_download = True), \n",
    "    'tokenizer': BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased'),\n",
    "    'max_length': [MAX_LENGTH],\n",
    "    'batch_size': [8]},   \n",
    "}\n",
    "\n",
    "params = {\n",
    "    'freeze':False,\n",
    "    'truncate_text': FILTER_LEN,  #'truncate','filter_matching_length'                \n",
    "    'learning_rate': 1e-5,   \n",
    "    'activation_func': 'Softmax',  \n",
    "    'mask_entities': False,\n",
    "    'dropout': 0.001,\n",
    "    'batch_size': 8,\n",
    "    'weight_decay':0,  \n",
    "}\n",
    "\n",
    "config = {'label_col': 'first_party_winner',\n",
    "          'text_col': TEXT_COL,\n",
    "          'epochs':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94a6320d-3bb0-488a-be98-876062ab67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(parameters,models_conf,config,base_df):\n",
    "    \n",
    "    df_tmp = base_df\n",
    "    results = {}\n",
    "    model_num = 0\n",
    "    params_hash = dict_hash(parameters)\n",
    "\n",
    "\n",
    "    for model_name, model_objects in models_conf.items():\n",
    "        results = {}\n",
    "        for max_length, dataset_batch_size in zip(model_objects['max_length'], model_objects['batch_size']):\n",
    "            tokenizer = model_objects['tokenizer']\n",
    "            pretrained_model =model_objects['model']\n",
    "            \n",
    "            df = df_tmp.reset_index()\n",
    "                                    \n",
    "            model = TextClassifier(model=pretrained_model, num_labels=df[config['label_col']].nunique(), num_features=768,dropout=parameters['dropout'],freeze_layers=parameters['freeze'],activation_func = parameters['activation_func'] )\n",
    "            \n",
    "            \n",
    "            case_ids_train, case_ids_test = train_test_split(\n",
    "            df.ID.drop_duplicates(), test_size=0.35, stratify=df[['ID',config['label_col']]].drop_duplicates()[config['label_col']], random_state=int(model_name.split('_')[-1]))\n",
    "\n",
    "            df_test_tmp = df[df.ID.isin(case_ids_test.tolist())]\n",
    "\n",
    "            case_ids_val, case_ids_test = train_test_split(case_ids_test, test_size=0.5, stratify=df_test_tmp[['ID',config['label_col']]].drop_duplicates()[config['label_col']], random_state=int(model_name.split('_')[-1]))\n",
    "\n",
    "            df_train = df[df.ID.isin(case_ids_train.tolist())]\n",
    "            df_test= df[df.ID.isin(case_ids_test.tolist())]\n",
    "            df_val= df[df.ID.isin(case_ids_val.tolist())]\n",
    "            \n",
    "            \n",
    "            if parameters['truncate_text'] == 'filter_matching_length':\n",
    "                df_train, df_test, df_val = filter_match_len(df_train,df_test,df_val,tokenizer,max_length)\n",
    "            \n",
    "            \n",
    "\n",
    "            config['max_length']=max_length\n",
    "            config['batch_size']=dataset_batch_size\n",
    "            \n",
    "            # train\n",
    "            max_epoch,results[f'{model_name}_{max_length}'] = train(model, tokenizer,df_train,df_val, df_test, config,parameters,model_num=model_num,model_name=model_name,accum_iter = parameters['batch_size']/dataset_batch_size)\n",
    "            \n",
    "            results[f'{model_name}_{max_length}']['params'] = parameters\n",
    "            results[f'{model_name}_{max_length}']['label_dist'] = {'train':normalized_counter(Counter(df_train[config['label_col']])),\n",
    "                                            'test':normalized_counter(Counter(df_test[config['label_col']])),\n",
    "                                            'val':normalized_counter(Counter(df_val[config['label_col']]))}\n",
    "            \n",
    "            best_epoch = config['epochs'] if max_epoch == config['epochs'] else max_epoch-2\n",
    "            results[f'{model_name}_{max_length}']['best_epoch'] = best_epoch        \n",
    "            model_num += 1\n",
    "            \n",
    "\n",
    "            val = Dataset(df_val, config['label_col'], config['text_col'], config['max_length'],tokenizer)\n",
    "            val_dataloader = torch.utils.data.DataLoader(val, batch_size=1)    \n",
    "                                    \n",
    "            torch.cuda.empty_cache()\n",
    "            del model\n",
    "            del tokenizer\n",
    "            del pretrained_model\n",
    "           \n",
    "            print(results[f'{model_name}_{max_length}'][max_epoch]['test']['accuracy'])\n",
    "            \n",
    "        print('finished',model_name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f04d8001-c55a-426d-b128-8f0166e17750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00cbba7f-cbd1-4a6d-afdc-05b70f8fc963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e0e3f0ca264048b824eb885da72089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger Times: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c0356d4e9141d8a8f8b30dd816a684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger Times: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442fe63540374845b2f651d6825cafdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger Times: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165c8f24c15f405fa1d1931dede6bea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger Times: 2\n",
      "Early Stopping!\n",
      "Start to test process.\n",
      "0.5852534562211982\n",
      "finished LegalBert_512_1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_performance = train_and_evaluate(params,models_conf,config,base_df = train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e95f5-3aca-4ef0-a2a7-ebec3804a475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
